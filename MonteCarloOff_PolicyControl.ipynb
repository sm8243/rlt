{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBRzaPYQfu6L",
        "outputId": "c9ee7d57-9979-4a02-8a65-50f545c5f054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q Values: {('S', 'left'): 0, ('S', 'right'): 0, ('A', 'left'): 0, ('A', 'right'): 0, ('B', 'left'): 0, ('B', 'right'): 0, ('C', 'left'): 0, ('C', 'right'): 0.0, ('G', 'left'): 0, ('G', 'right'): 0}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.states = ['S', 'A', 'B', 'C', 'G']  # States: S - Start, A, B, C, G - Goal\n",
        "        self.num_states = len(self.states)\n",
        "        self.actions = ['left', 'right']  # Actions: left, right\n",
        "        self.num_actions = len(self.actions)\n",
        "        self.transitions = {\n",
        "            'S': {'left': 'S', 'right': 'A'},\n",
        "            'A': {'left': 'S', 'right': 'B'},\n",
        "            'B': {'left': 'A', 'right': 'C'},\n",
        "            'C': {'left': 'B', 'right': 'G'},\n",
        "            'G': {}\n",
        "        }  # Transition function\n",
        "        self.rewards = {\n",
        "            'S': {'left': 0, 'right': 0},\n",
        "            'A': {'left': 0, 'right': 0},\n",
        "            'B': {'left': 0, 'right': 0},\n",
        "            'C': {'left': 0, 'right': 0},\n",
        "            'G': {}\n",
        "        }  # Reward function\n",
        "\n",
        "    def reset(self):\n",
        "        return 'S'  # Reset to initial state 'S'\n",
        "\n",
        "    def step(self, state, action):\n",
        "        next_state = self.transitions[state][action]\n",
        "        reward = self.rewards[state][action]\n",
        "        done = (next_state == 'G')  # Terminates if reaching the goal state 'G'\n",
        "        return next_state, reward, done\n",
        "\n",
        "class MonteCarloOffPolicyControl:\n",
        "    def __init__(self, env, num_episodes, gamma, behavior_policy, target_policy, epsilon):\n",
        "        self.env = env\n",
        "        self.num_episodes = num_episodes\n",
        "        self.gamma = gamma\n",
        "        self.behavior_policy = behavior_policy\n",
        "        self.target_policy = target_policy\n",
        "        self.epsilon = epsilon\n",
        "        self.q_values = {(s, a): 0 for s in env.states for a in env.actions}\n",
        "        self.c_values = {(s, a): 0 for s in env.states for a in env.actions}\n",
        "\n",
        "    def generate_episode(self):\n",
        "        episode = []\n",
        "        state = self.env.reset()\n",
        "        while True:\n",
        "            action = np.random.choice(self.env.actions, p=self.behavior_policy[state])\n",
        "            next_state, reward, done = self.env.step(state, action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "        return episode\n",
        "\n",
        "    def update_q_values(self, episode):\n",
        "        G = 0\n",
        "        weight = 1\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = self.gamma * G + reward\n",
        "            self.c_values[(state, action)] += weight\n",
        "            self.q_values[(state, action)] += (weight / self.c_values[(state, action)]) * (G - self.q_values[(state, action)])\n",
        "            if action != self.target_policy[state]:\n",
        "                break\n",
        "            weight /= self.behavior_policy[state][action]\n",
        "\n",
        "    def run(self):\n",
        "        for _ in range(self.num_episodes):\n",
        "            episode = self.generate_episode()\n",
        "            self.update_q_values(episode)\n",
        "\n",
        "# Example usage:\n",
        "env = Environment()\n",
        "# Define behavior policy (e.g., epsilon-greedy)\n",
        "behavior_policy = {'S': [0.5, 0.5], 'A': [0.5, 0.5], 'B': [0.5, 0.5], 'C': [0.5, 0.5], 'G': []}\n",
        "# Define target policy (e.g., greedy)\n",
        "target_policy = {'S': 1, 'A': 1, 'B': 1, 'C': 1, 'G': []}\n",
        "mc_off_policy_control = MonteCarloOffPolicyControl(env, num_episodes=1000, gamma=0.9, behavior_policy=behavior_policy, target_policy=target_policy, epsilon=0.1)\n",
        "mc_off_policy_control.run()\n",
        "print(\"Q Values:\", mc_off_policy_control.q_values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid world\n",
        "GRID_SIZE = 5\n",
        "START_STATE = (0, 0)\n",
        "GOAL_STATE = (GRID_SIZE-1, GRID_SIZE-1)\n",
        "OBSTACLES = [(1, 1), (2, 2), (3, 3)]\n",
        "ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up\n",
        "\n",
        "# Define the SARSA parameters\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "num_episodes = 1000\n",
        "\n",
        "# Initialize the Q-values\n",
        "Q = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
        "\n",
        "# Define the epsilon-greedy policy\n",
        "def epsilon_greedy_policy(state, Q, epsilon):\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        return np.random.randint(len(ACTIONS))  # Explore action space\n",
        "    else:\n",
        "        return np.argmax(Q[state])  # Exploit learned values\n",
        "\n",
        "# Run SARSA\n",
        "for _ in range(num_episodes):\n",
        "    state = START_STATE\n",
        "    action = epsilon_greedy_policy(state, Q, epsilon)\n",
        "    while state != GOAL_STATE:\n",
        "        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])\n",
        "        if next_state in OBSTACLES or not(0 <= next_state[0] < GRID_SIZE) or not(0 <= next_state[1] < GRID_SIZE):\n",
        "            next_state = state  # Stay in the current state if hitting an obstacle or out of bounds\n",
        "        next_action = epsilon_greedy_policy(next_state, Q, epsilon)\n",
        "        reward = 1 if next_state == GOAL_STATE else 0\n",
        "        Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
        "        state = next_state\n",
        "        action = next_action\n",
        "\n",
        "# Print the learned Q-values\n",
        "for i in range(GRID_SIZE):\n",
        "    for j in range(GRID_SIZE):\n",
        "        print(f\"State: ({i}, {j}) - Q-values: {Q[(i, j)]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "use6RKgZf8S5",
        "outputId": "c9b65bd5-9a06-477e-c76d-2dc626b772aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: (0, 0) - Q-values: [0.42526003 0.31670972 0.17809119 0.32334191]\n",
            "State: (0, 1) - Q-values: [0.49507268 0.32651324 0.3596054  0.37174281]\n",
            "State: (0, 2) - Q-values: [0.54198916 0.34742212 0.23641026 0.40073594]\n",
            "State: (0, 3) - Q-values: [0.63133494 0.42311165 0.40308966 0.46904331]\n",
            "State: (0, 4) - Q-values: [0.57232147 0.4578545  0.71185721 0.52160307]\n",
            "State: (1, 0) - Q-values: [0.01546739 0.04315087 0.         0.32468693]\n",
            "State: (1, 1) - Q-values: [0. 0. 0. 0.]\n",
            "State: (1, 2) - Q-values: [0.05643965 0.         0.02429257 0.36786944]\n",
            "State: (1, 3) - Q-values: [0.67497889 0.00145801 0.         0.0533865 ]\n",
            "State: (1, 4) - Q-values: [0.62766764 0.53323226 0.79552496 0.51813145]\n",
            "State: (2, 0) - Q-values: [0. 0. 0. 0.]\n",
            "State: (2, 1) - Q-values: [0. 0. 0. 0.]\n",
            "State: (2, 2) - Q-values: [0. 0. 0. 0.]\n",
            "State: (2, 3) - Q-values: [0.71387529 0.         0.         0.0597241 ]\n",
            "State: (2, 4) - Q-values: [0.65411319 0.46702935 0.88283955 0.60651625]\n",
            "State: (3, 0) - Q-values: [0. 0. 0. 0.]\n",
            "State: (3, 1) - Q-values: [0. 0. 0. 0.]\n",
            "State: (3, 2) - Q-values: [0. 0. 0. 0.]\n",
            "State: (3, 3) - Q-values: [0. 0. 0. 0.]\n",
            "State: (3, 4) - Q-values: [0.79694861 0.8082189  1.         0.68298757]\n",
            "State: (4, 0) - Q-values: [0. 0. 0. 0.]\n",
            "State: (4, 1) - Q-values: [0. 0. 0. 0.]\n",
            "State: (4, 2) - Q-values: [0. 0. 0. 0.]\n",
            "State: (4, 3) - Q-values: [0.1 0.  0.  0. ]\n",
            "State: (4, 4) - Q-values: [0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1zlDjW0Jg117"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}