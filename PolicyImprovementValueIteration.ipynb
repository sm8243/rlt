{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfN2RJ3nVedT",
        "outputId": "090cdaeb-0eeb-4a33-e66d-22d6a98b6aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create the FrozenLake environment\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "\n",
        "# Initialize value function and policy\n",
        "V = np.zeros(env.observation_space.n)\n",
        "policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "\n",
        "# Discount factor\n",
        "gamma = 0.99\n",
        "\n",
        "# Policy evaluation\n",
        "def evaluate_policy():\n",
        "    delta = 1e-6\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(env.observation_space.n):\n",
        "            v = V[s]\n",
        "            V[s] = sum([p * (r + gamma * V[s_]) for p, s_, r, _ in env.P[s][policy[s]]])\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < 1e-6:\n",
        "            break\n",
        "\n",
        "# Policy improvement\n",
        "def improve_policy():\n",
        "    policy_stable = True\n",
        "    for s in range(env.observation_space.n):\n",
        "        old_action = policy[s]\n",
        "        policy[s] = np.argmax([sum([p * (r + gamma * V[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.action_space.n)])\n",
        "        if old_action != policy[s]:\n",
        "            policy_stable = False\n",
        "    return policy_stable\n",
        "\n",
        "# Value iteration\n",
        "while True:\n",
        "    evaluate_policy()\n",
        "    if improve_policy():\n",
        "        break\n",
        "\n",
        "# Print optimal value function and policy\n",
        "print(\"Optimal Value Function:\")\n",
        "print(V.reshape((4, 4)))\n",
        "print(\"Optimal Policy (0=Left, 1=Down, 2=Right, 3=Up):\")\n",
        "print(policy.reshape((4, 4)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m28d48DWJmz",
        "outputId": "67eea6a8-44f3-49f0-d541-1f710f3cc563"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value Function:\n",
            "[[0.54201384 0.49878716 0.47067695 0.45683159]\n",
            " [0.55844022 0.         0.35833998 0.        ]\n",
            " [0.59178998 0.64307352 0.61520205 0.        ]\n",
            " [0.         0.7417161  0.86283524 0.        ]]\n",
            "Optimal Policy (0=Left, 1=Down, 2=Right, 3=Up):\n",
            "[[0 3 3 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DP- Policy Improvement and Value Iteration\n",
        "import numpy as np\n",
        "\n",
        "class MDP:\n",
        "    def __init__(self, num_states, num_actions, transition_probs, rewards, gamma=0.9):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.transition_probs = transition_probs  # shape: (num_states, num_actions, num_states)\n",
        "        self.rewards = rewards  # shape: (num_states, num_actions)\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def policy_evaluation(self, policy, tol=1e-6):\n",
        "        V = np.zeros(self.num_states)\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(self.num_states):\n",
        "                v = V[s]\n",
        "                action = policy[s]\n",
        "                V[s] = sum(self.transition_probs[s, action, s_prime] *\n",
        "                           (self.rewards[s, action] + self.gamma * V[s_prime])\n",
        "                           for s_prime in range(self.num_states))\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "            if delta < tol:\n",
        "                break\n",
        "        return V\n",
        "\n",
        "    def policy_improvement(self, V):\n",
        "        policy = np.zeros(self.num_states, dtype=int)\n",
        "        for s in range(self.num_states):\n",
        "            policy[s] = np.argmax(self._bellman_operator(s, V))\n",
        "        return policy\n",
        "\n",
        "    def value_iteration(self, tol=1e-6):\n",
        "        V = np.zeros(self.num_states)  # Initialize value function\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(self.num_states):\n",
        "                v = V[s]\n",
        "                V[s] = max(self._bellman_operator(s, V))\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "            if delta < tol:\n",
        "                break\n",
        "        policy = self.policy_improvement(V)\n",
        "        return V, policy\n",
        "\n",
        "    def _bellman_operator(self, state, V):\n",
        "        Q = np.zeros(self.num_actions)\n",
        "        for a in range(self.num_actions):\n",
        "            for s_prime in range(self.num_states):\n",
        "                Q[a] += self.transition_probs[state, a, s_prime] * (self.rewards[state, a] + self.gamma * V[s_prime])\n",
        "        return Q\n",
        "\n",
        "# Example usage\n",
        "num_states = 3\n",
        "num_actions = 2\n",
        "transition_probs = np.array([[[0.5, 0.5, 0.0], [1.0, 0.0, 0.0]],\n",
        "                             [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]],\n",
        "                             [[0.0, 1.0, 0.0], [0.5, 0.5, 0.0]]])\n",
        "rewards = np.array([[1.0, 2.0], [0.0, 0.0], [5.0, -1.0]])\n",
        "\n",
        "mdp = MDP(num_states, num_actions, transition_probs, rewards)\n",
        "V, policy = mdp.value_iteration()\n",
        "print(\"Optimal value function:\", V)\n",
        "print(\"Optimal policy:\", policy)\n"
      ],
      "metadata": {
        "id": "wyncOhgXWZjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378361d4-5808-4f10-ebbb-a61afcccaeee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal value function: [21.1961682  23.68420728 26.31578656]\n",
            "Optimal policy: [0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6d0n4Nr6HMY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}